from fastapi import FastAPI, UploadFile, File
import azure.functions as func
import pandas as pd
from sqlalchemy import create_engine
import urllib
import pyodbc
import os
import datetime
from dotenv import load_dotenv
import logging

# Load environment variables from .env file
load_dotenv(dotenv_path=".env")

# These variables should be defined globally AFTER load_dotenv()
server = os.getenv("AZURE_SERVER")
database = os.getenv("AZURE_DATABASE")
username = os.getenv("AZURE_USERNAME")
password = os.getenv("AZURE_PASSWORD")
db_connection = os.getenv("DB_CONNECTION_STRING")

#Set up logging for debugging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Validate environment variables (This check should happen *after* variables are defined)
if not all([server, database, username, password]):
    raise ValueError("Missing database credentials in environment variables!")

if db_connection == "placeholder_value":
    logger.info("Running in local test mode...")

# Azure Function Entry Point (currently unused by FastAPI app)
def main(req: func.HttpRequest) -> func.HttpResponse:
    try:
        req_body = req.get_json()
        return func.HttpResponse(f"Received request: {req_body}", status_code=200)
    except Exception as e:
        return func.HttpResponse(f"Error processing request: {str(e)}", status_code=400)

# FastAPI for Local Testing
app = FastAPI()

@app.post("/upload-csv/")
async def upload_csv(file: UploadFile = File(...)):
    filename = file.filename.lower()
    logger.info(f"ðŸ“‚ Received file: {filename}")

    # Step 1: Read CSV with error handling
    try:
        df = pd.read_csv(file.file)
    except Exception as e:
        logger.error(f"Failed to read CSV file: {str(e)}")
        return {"error": f"Failed to read CSV file: {str(e)}"}

    # Step 2: Route to table + schema based on file name
    if "football" in filename:
        expected_headers = ['PlayerID', 'Position', 'Yards', 'Touchdowns', 'GameDate']
        target_table = 'football_stats'
    elif "basketball" in filename:
        expected_headers = ['AthleteID', 'Name', 'Points', 'Rebounds', 'Assists', 'GameDate']
        target_table = 'basketball_performance'
    else:
        logger.warning(f"Unknown or unsupported sport type in file name: {filename}")
        return {"error": "Unknown or unsupported sport type in file name."}

    # Step 3: Validate headers
    if not all(header in df.columns for header in expected_headers):
        logger.error(f"CSV headers mismatch for {target_table}. Expected: {expected_headers}, Received: {df.columns.tolist()}")
        return {
            "error": f"CSV headers do not match expected format for {target_table}.",
            "expected": expected_headers,
            "received": df.columns.tolist()
        }

    # Step 4: Clean and format data
    df_clean = df[expected_headers]
    if 'GameDate' in df_clean.columns:
        try:
            df_clean['GameDate'] = pd.to_datetime(df_clean['GameDate'])
        except Exception as e:
            logger.error(f"Error converting GameDate to datetime: {str(e)}")
            return {"status": "error", "message": f"Error converting GameDate: {str(e)}"}
        
    logger.info(f"ðŸšš Routing data to table: {target_table}")
    logger.info(df_clean.head())

    # Step 5: Insert into Azure SQL Database
    try:
        driver = '{ODBC Driver 17 for SQL Server}'
        # Ensure server, database, username, password are correctly loaded
        # from global variables (which come from os.getenv)
        
        # This string must contain ALL ODBC attributes expected by pyodbc
        odbc_connection_attributes = (
            f'DRIVER={driver};'
            f'SERVER={server},1433;' 
            f'DATABASE={database};'
            f'UID={username};'
            f'PWD={password};'
            f'Encrypt=yes;' 
            f'TrustServerCertificate=no;' 
            f'Connection Timeout=30;' 
        )
        logger.info(f"ODBC attributes string (before quoting): {odbc_connection_attributes}") # DEBUGGING LOG
        
        conn_str = f"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(odbc_connection_attributes)}"
        logger.info(f"Final SQLAlchemy connection string: {conn_str}") # DEBUGGING LOG

        engine = create_engine(conn_str, fast_executemany=True)

        with engine.begin() as conn:
            df_clean.to_sql(target_table, con=conn, if_exists='append', index=False)
        logger.info(f"âœ… Successfully uploaded {len(df_clean)} rows to {target_table}.")

    except Exception as e:
        logger.error(f"Database insert failed: {str(e)}")
        return {"status": "error", "message": f"Database insert failed: {str(e)}"}

    return {
        "status": "success",
        "rows_uploaded": len(df_clean),
        "routed_to": target_table
    }


=======================================================================================================================================================
curl setup:
C:\Windows\System32\curl.exe -X POST "http://127.0.0.1:8000/upload-csv/" `
     -H "accept: application/json" `
     -H "Content-Type: multipart/form-data" `
     -F "file=@ {YOUR FILE GOES HERE}"



